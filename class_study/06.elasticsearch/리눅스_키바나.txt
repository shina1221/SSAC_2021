#vmware workstation pro 전체화면 보기 ctrl+alt+enter
#혹 vmware tool이라는 것을 설치 혹은 
#system -p
#리눅스 키바나 사용

#도큐먼트 추가
put my_index/_doc/1
{
  "name": "najeong Shin",
  "messge": "안녕하세요 Elasticsearch"
}

#도큐먼트 읽어들이기
GET my_index/_doc/1

#도큐먼트 삭제하기
DELETE my_index/_doc/1
or DELETE my_index

#위에서 지워서 다시 도큐먼트 추가
put my_index/_doc/1
{
  "name": "najeong Shin",
  "messge": "안녕하세요 Elasticsearch"
}

#PUT은 기존에 데이터가 있을 경우 전체 overwrite
#post는 기존에 바꿀 부분만 갱신

#지웠던 도큐먼트 갱신
#PUT
PUT my_index/_doc/1
{
  "age": 40
}

#POST
POST my_index/_doc/1
{
  "doc": {
    "age": 40
  }
}
GET my_index/_doc/1

#데이터들을 한번에 넣기
POST _bulk
{"index" : {"_index": "test", "_id": "1"}}
{"field": "value one"}
{"index" : {"_index": "test", "_id": "2"}}
{"field": "value two"}
{"delete": {"_index": "test", "_id": "2"}}
{"create": {"_index": "test", "_id": "3"}}
{"field": "value three"}
{"update": {"_index": "test", "_id": "1"}}
{"doc": {"field": "value two"}}

GET test/_doc/1
GET test/_doc/2
GET test/_doc/3

#데이터 검색
GET test/_search?q=two
GET test/_search?q=three
GET test/_search?q=field:three AND field:value

GET test/_search
{
  "query": {
    "match": {
      "message": "안녕하세요"
    }
  }
}

#로그스태시는 외부에서 얻는 데이터를 필터링을 거쳐 필요한 데이터만 얻은뒤 엘라스틱 서치에 넣을때 사용
#로그스태시 설치(노드 1번에) 설치는 반드시 홈디렉토리에서
wget https://artifacts.elastic.co/downloads/logstash/logstash-7.14.1-linux-x86_64.tar.gz

tar xfz logstash-7.14.1-linux-x86_64.tar.gz

#실행확인
#input값을 그대로 출력하기
~/logstash-7.14.1/bin/logstash -e 'input { stdin { } } output { stdout { } }'

#아래의 디렉토리로 가서
cd ~/logstash-7.14.1/config

#파일 생성
test.conf라는 파일 생성

#다음의 내용 conf 파일에 넣기
#여기서 명령어를 파일내에 넣어서 실행하는 것임
input { stdin{ } }
output { stdout{ } }

#실행하기
~/logstash-7.14.1/bin/logstash -f test.conf

#1
#새로운 터미널 창 실행해서 test.conf 파일을 열어서 수정 후 재실행
#9900번 포트에서 들어오는 데이터를 인풋으로 쓰겠다는 의미

gedit test.conf

input {
	tcp {
		port => 9900
	}
}

output {
	stdout { }
}
 
#프로그램이 실행되는데 시간이 좀 오래걸림
~/logstash-7.14.1/bin/logstash -f test.conf

# 새로운 터미널 창을 열어 새로운 터미널 실행
#로컬호스트 9900번 포트에 다음의 내용을 보내겠다는 의미
echo 'hello logstash' | nc localhost 9900
#해당 내용을 보낸 후 원래 logstash를 실행했던 터미널을 보면 출력이 된것을 확인할 수 있음.
 
#2
#로컬 호스트의 9200(엘라스틱서치)에 넣는데 
gedit test.conf

input {
	tcp {
		port => 9900
	}
}

output {
	elasticsearch {
		hosts => ["localhost:9200"]
		user => "elastic"
		password => "bigdata"
    }
}

~/logstash-7.14.1/bin/logstash -f test.conf

#새로운 터미널에서
echo 'hello logstash' | nc localhost 9900

#노드3번 키바나실행 후 dev tools에서 다음의 명령어를 치면 
#로그스태시 내용이 던져진게 보일것임
GET _cat/indices
GET logstash-*/_search

#3
#우선 데이터 다운로드해서 weblog-sample파일 옮기기
#vmware는 윈도우에서 vmware 리눅스로 바로 파일을 (복사 붙여넣기로)옮겨 붙이는것이 가능
#위치는 홈디렉토리로(맨 처음 디렉토리)

#파일내용 확인
head ~/weblog-sample.log

#output에 필터적용하기
#grok을 이용해 매칭되는것을 추출

#아파치는 보통 일레스텍서치에서 웹로그를 많이 분석하기 때문에 분석하는 기능을 넣어둠
#해당 부분은 COMBINEDAPACHELOG로 확인

gedit test.conf

input {
	tcp {
		port => 9900
	}
}

filter {
    grok {
    match => { "message" => "%{COMBINEDAPACHELOG}"}
  }
}

output {
	stdout { }
}


~/logstash-7.14.1/bin/logstash -f test.conf

#새로운 터미널 실행
#웹로그를 이용할 것임
#파일을 읽어들여서 1행만 읽어들어서 볼것임
head -n 1 ./weblog-sample.log | nc localhost 9900
#다음의 명령어를 실행하면 로그스태시에서 확인가능

#head ~/weblog-sample.log 1행
#"GET /articles/ppp-over-ssh/ HTTP/1.1" 200 18586 "-" "Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.2b1) Gecko/20091014 Firefox/3.6b1 GTB5"

#4
#기존에 생성해둔게 있어서 노드 3에서 인덱스 지워야함
#노드3의 키바나에서
DELETE logstash*

#conf파일 변경 노드1에서 작성
#mutate로 데이터 타입을 byte에서 int로 변경할 것임
gedit test.conf
input {
	tcp {
		port => 9900
	}
}

filter {
    grok {
     match => { "message" => "%{COMBINEDAPACHELOG}" }
   }
    geoip {
     source => "clientip"
   }
   useragent {
    source => "agent"
    target => "useragent"
   }
   mutate {
    convert => {
      "bytes" => "integer"
     }
   }
   date {
     match => ["timestamp", "dd/MMM/yyyy:HH:mm:ss Z"]
     target => "logdate"
  }
}

output {
	elasticsearch {
		hosts => ["localhost:9200"]
		user => "elastic"
		password => "bigdata"
       }
}


~/logstash-7.14.1/bin/logstash -f test.conf

#새로운 터미널 실행
head -n 1 ./weblog-sample.log | nc localhost 9900

#키바나에서
GET _cat/indices
GET logstash*/_search

#5 
#한글 형태소 분석
#아래의 파일 모든 노드에 설치 이후 재부팅
~/es-714/bin/elasticsearch-plugin install analysis-nori

#이후에 엘라스틱서치 키바나 전부 재실행
#밑의 쿼리는 숫자 안바꾸어 어느 노드에서든 돌려도 됨
curl "elastic-1:9200/_cat/nodes?v" -u elastic:bigdata
#노드가 전부 올라와있는지 확인


#한글버전 
settings > region & language > Input source korean(hangul) add
                                        > Language 한국어
설정후 restart > logout   
예전이름 유지 반드시 누를 것 이거 안누르면 파일명 전부 바뀔것임

#현 리눅스에서 윈도우키+shift+space_bar하면 한글로 변환가능           

#키바나에서 해보기 
#1
#nori 라는 한글 형태소 분석기를 활용해 텍스트 분석
GET _analyze
{
    "analyzer": "nori",
    "text": [ "동해물과 백두산이" ]
}

#2
#my_nori라는 인덱스 생성
#토크나이저를 통해 특정 단어를 인식할 수 있음
#my_nori 인덱스에 "해물"사전을 추가한 my_nori_tokenizer 생성
PUT my_nori
{
  "settings": {
    "analysis": {
      "tokenizer": {
        "my_nori_tokenizer": {
          "type": "nori_tokenizer",
          "user_dictionary_rules": [
            "해물"
          ]
        }
      }
    }
  }
}
                                          
#my_nori_tokenizer 토크나이저로 "동해물과" 분석하기
GET my_nori/_analyze
{
  "tokenizer": "my_nori_tokenizer",
  "text": [
    "동해물과"
  ]
}

#3
#elasticsearch nori 분석기 서치
#gh402.tistory.com/49 참조

#앞에서 이미 인덱스를 만들어뒀으므로 지워야함.
DELETE my_nori
PUT my_nori
{
  "settings": {
    "analysis": {
      "tokenizer": {
        "nori_none": {
          "type": "nori_tokenizer",
          "decompound_mode": "none"
        },
        "nori_discard": {
          "type": "nori_tokenizer",
          "decompound_mode": "discard"
        },
        "nori_mixed": {
          "type": "nori_tokenizer",
          "decompound_mode": "mixed"
        }
      }
    }
  }
}
 
#
GET my_nori/_analyze
{
  "tokenizer": "nori_none",
  "text": [ "동해물과" ]
}

GET my_nori/_analyze
{
  "tokenizer": "nori_discard",
  "text": [ "동해물과" ]
}

GET my_nori/_analyze
{
  "tokenizer": "nori_mixed",
  "text": [ "동해물과" ]
}

#4
#숫자제거하기 
#NR은 숫자를 의미
#다섯이라는 단어까지도 숫자로 인식

PUT my_pos
{
  "settings": {
    "index": {
      "analysis": {
        "filter": {
          "my_pos_f": {
            "type": "nori_part_of_speech",
            "stoptags": [
              "NR"
            ]
          }
        }
      }
    }
  }
}

#my_pos_f 토큰필터로 "다섯아이가" 분석 
#다섯 제외
GET my_pos/_analyze
{
  "tokenizer": "nori_tokenizer",
  "filter": [
    "my_pos_f"
  ],
  "text": "다섯아이가" 
}

#filebeat
#노드1에 설치
wget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.14.1-linux-x86_64.tar.gz
tar xfz filebeat-7.14.1-linux-x86_64.tar.gz
mv filebeat-7.14.1-linux-x86_64.tar.gz files
mv filebeat-7.14.1-linux-x86_64 fb-714

#yml파일을 열어서 설정 변경
gedit ~/fb-714/filebeat.yml

#아래의 내용으로 변경(이대로 붙여넣지 말것/ 기존의 것은 그냥 주석차리했음 )
#위하고 아래에 같은 부분이 있을텐데 윗부분에만 적용(Multiline options 부문은 건드리지 않음)

# Change to true to enable this input configuration.
enabled: true
# Paths that should be crawled and fetched. Glob based paths.
  paths:
    - /home/elastic-1/weblog-sample.log
...
output.elasticsearch:
  #Array of hosts to connect to.
  hosts: ["localhost:9200"]

  # Authentication credentials - either API key or username/password.
  #api_key: "id:api_key"
  username: "elastic"
  password: "bigdata"

#노드1에서  
cd fb-714
./filebeat

#노드 3 devtools가서
GET _cat/indices
GET filebeat-7.14.1*/_search
#message 필드 확인

#elastic-1 (옵션 해도되고 안해도 되고)
Filebeat 중지 (ctrl+c)

#elastic-3 키바나
#로그 파일 데이터를 전부 적재할 것임
#적재할 때마다 count 숫자 올라갈것임
GET filebeat-7.14.1*/_count
#나중에 필요없어지면 지우면 됨
DELETE filebeat*

#대시보드 
#코로나 데이터로 해볼것임
#키바나가 있는 노드3 홈디렉토리에 coviddata폴더 생성해서 파일 5개 복붙

#키바나 홈에서 Upload a file 선택
#csv파일이면 대부분 활용가능
#100MB이상은 지금 방법으로 안됨
#time부터
# import
#import data > Advanced
#Index name : covid-kr-time
#create index pattern 체크 해제
#Mappings에서 long으로 된 데이터타입은 integer로 전부 변경
#Mappings 맨 마지막 time의 type은 byte로 변경
#데이터 타입 변경이 완료되면 왼쪽 하단의 import 클릭

#dev tools가서 아래의 명령어로 제대로 되었는지 확인
GET /covid-kr-time
GET /covid-kr-time/_search

#키바나 stack management > index patterns > create index pattern 클릭
#index pattern name에 covid-kr-time 지정 후 next step
#time field는 @timestamp 선택 후 create index pattern

#menu > discover
#현재 데이터가 2020년도 데이터이므로 
#absolute 2020.01.01 ~ 2020.03.31 update 후 기준은 day로 확인

#목록에서 visualize library
#시각화 결과물을 이곳에서 만듦
#이후에 대시보드에 띄움

visualize library>create new visualization>aggregation based>Line>covid_kr_time
#옵션지정
buckets > add > x-axis -aggregation(data histogram), -minimum interval(day) > update
#metrics > y-axis -aggregation(max), -field(test), custom label(test) >update
metrics > y-axis -aggregation(max), -field(negative) >update
metrics > y-axis -aggregation(max), -field(confirmed) , custom label(comfirmed), 
metrics > y-axis max deceased, deceased

#matrics&axes 
Y
leftaxes 
left normal square root
title : number of people
#show labels > save

#save로 간 뒤
title covid-kr:number of inspectors > none > save and go to dashboard

#main dashboard> create dashboard > add from library> covid-kr:number of inspectors
#이후에 또 저장해줘야함

#각각의 시각화 라이브러리를 저장하고 대시보드라는 파일에 시각화 결과물을 불러오고 그것을 저장해야함. 
#묶어서 저장하기 위해

#추가적인 필터링
add filter 
disable로 필터적용 미적용 가능


#참고
https://www.youtube.com/watch?v=DzGwmr8nKPg&t=10s

#참고
https://www.youtube.com/channel/UCIy5GtVvLEiLik0T2bZwm7g


