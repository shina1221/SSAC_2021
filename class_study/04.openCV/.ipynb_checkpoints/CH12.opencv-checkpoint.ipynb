{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5b2456",
   "metadata": {},
   "outputs": [],
   "source": [
    "##8장 영상매칭과 추적\n",
    "\"\"\"\n",
    "영상매칭:서로 다른 두 영상을 비교해서 영상 속 객체가 같은 것인지를 알아내거나 여러 영상 중에서 짝이 맞는 영상을 찾아내는 것\n",
    "영상 매칭으로 객체를 인식하는 방법은 영상에서 의미 있다고 판단하는 특징들을 적절한 숫자로 변환하고 그 숫자들을 비교해서 얼마나 비슷한지 판단하는 것\n",
    "\n",
    "디스크립터:영상의 특징으로 대표할 수 있는 숫자를 특징 벡터 또는 특징\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85a7a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#평균 해시 매칭\n",
    "\"\"\"\n",
    "평균해시 :어떤 영상이든 동일한 크기의 하나의 숫자로 변환되는데, 이때 숫자를 얻기 위해 평균 값을 이용한다는 것\n",
    "평균을 얻기 전에 영상을 가로 세로 비율과 무관하게 특정한 크기로 축소\n",
    "이후 픽셀 전체의 평균 값을 구해서 각 픽셀의 값이 평균보다 작으면 0, 크면 1로 변환\n",
    "이후 0또는 1로만 구성된 각 픽셀 값을 1행 1열로 변환\n",
    "2진수가 보기 불편하다면 10진수나 16진수로도 변환가능\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1446406f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 1 0 0 1 1 1 1 1 1]\n",
      " [1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1]\n",
      " [1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1]]\n",
      "ffff8000800080008000813fc1ffc1ffc07fc3ffc7ffc7ff87ff87ff87ffc7ff\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#권총을 평균 해시로 변환\n",
    "import cv2\n",
    "\n",
    "#영상을 읽어서 그레이 스케일로 변환\n",
    "img =cv2.imread('./img/pistol.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#16x16 크기로 축소\n",
    "gray = cv2.resize(gray, (16,16))\n",
    "#영상의 평균 값 구하기\n",
    "avg = gray.mean()\n",
    "#평균값을 기준으로 0과 1로 변환\n",
    "bin = 1 * (gray > avg)\n",
    "print(bin)\n",
    "\n",
    "#2진수 문자열을 16진수 문자열로 변환\n",
    "dhash = []\n",
    "for row in bin.tolist():\n",
    "    s = ''.join([str(i) for i in row])\n",
    "    dhash.append('%02x'%(int(s,2)))\n",
    "dhash=''.join(dhash)\n",
    "print(dhash)\n",
    "\n",
    "cv2.imshow('pistol', img)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01228206",
   "metadata": {},
   "outputs": [],
   "source": [
    "#평균해시\n",
    "\"\"\"\n",
    "유사도를 측정하는 방법으로 대표적으로 유클리드 거리와 해밍거리가 있음\n",
    "\n",
    "유클리드 거리는 두 값의 차이로 거리계산\n",
    "5와 3,8비교  5-3=거리2 8-5=거리3\n",
    "거리가 더 작으므로 5와 더 비슷한 수는 3\n",
    "\n",
    "해밍거리는 두 값의 길이가 같아야 계산가능\n",
    "두 수의 같은 자리 값이 서로 다른 것이 몇개인지를 나타내는 것이 해밍거리\n",
    "12345기준 12354와 92345\n",
    "          123      2345\n",
    "92345가 자리기준 같은 수가 더 많으므로 더 유사\n",
    "\n",
    "유클리드 거리는 높은 자릿수가 다를수록 더 큰 거리로 인식하는 반면, 해밍거리는 각 자릿수의 차이의 개수로만 비교하므로 \n",
    "앞서 얻은 영상의 평균 해시값을 비교하는데는 유클리드거리보다는 해밍거리로 측정하는 것이 더 적합\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01b0a7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./img/101_ObjectCategories/101_ObjectCategories\\BACKGROUND_Google\\image_0398.jpg 0.234375\n",
      "./img/101_ObjectCategories/101_ObjectCategories\\binocular\\image_0011.jpg 0.23828125\n",
      "./img/101_ObjectCategories/101_ObjectCategories\\Faces_easy\\image_0419.jpg 0.2421875\n",
      "./img/101_ObjectCategories/101_ObjectCategories\\revolver\\image_0001.jpg 0.2421875\n",
      "./img/101_ObjectCategories/101_ObjectCategories\\revolver\\image_0015.jpg 0.24609375\n",
      "./img/101_ObjectCategories/101_ObjectCategories\\revolver\\image_0017.jpg 0.23828125\n",
      "./img/101_ObjectCategories/101_ObjectCategories\\revolver\\image_0018.jpg 0.1953125\n",
      "./img/101_ObjectCategories/101_ObjectCategories\\revolver\\image_0019.jpg 0.23828125\n",
      "./img/101_ObjectCategories/101_ObjectCategories\\revolver\\image_0021.jpg 0.171875\n",
      "./img/101_ObjectCategories/101_ObjectCategories\\revolver\\image_0022.jpg 0.21484375\n",
      "./img/101_ObjectCategories/101_ObjectCategories\\revolver\\image_0023.jpg 0.21875\n",
      "./img/101_ObjectCategories/101_ObjectCategories\\revolver\\image_0031.jpg 0.21875\n",
      "./img/101_ObjectCategories/101_ObjectCategories\\revolver\\image_0033.jpg 0.2421875\n",
      "./img/101_ObjectCategories/101_ObjectCategories\\revolver\\image_0034.jpg 0.23046875\n",
      "./img/101_ObjectCategories/101_ObjectCategories\\revolver\\image_0037.jpg 0.2421875\n",
      "./img/101_ObjectCategories/101_ObjectCategories\\revolver\\image_0064.jpg 0.18359375\n",
      "./img/101_ObjectCategories/101_ObjectCategories\\revolver\\image_0068.jpg 0.24609375\n",
      "./img/101_ObjectCategories/101_ObjectCategories\\revolver\\image_0072.jpg 0.203125\n",
      "./img/101_ObjectCategories/101_ObjectCategories\\revolver\\image_0081.jpg 0.23046875\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Can't convert object of type 'int' to 'str' for 'winname'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14008/1528911748.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdestroyWindow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdestroyAllWindows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Can't convert object of type 'int' to 'str' for 'winname'"
     ]
    }
   ],
   "source": [
    "#사물 영상중에서 권총 영상 찾기\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "#영상읽기 및 표시\n",
    "img = cv2.imread('./img/pistol.jpg')\n",
    "cv2.imshow('query', img)\n",
    "\n",
    "#비교할 영상들이 있는 경로\n",
    "search_dir = './img/101_ObjectCategories/101_ObjectCategories'\n",
    "\n",
    "#이미지를 16x16크기의 평균해시로 변환\n",
    "def img2hash(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.resize(gray, (16,16))\n",
    "    avg = gray.mean()\n",
    "    bi = 1 * (gray>avg)\n",
    "    return bi\n",
    "\n",
    "#해밍거리 측정 함수\n",
    "def hamming_distance(a,b):\n",
    "    a=a.reshape(1,-1)\n",
    "    b=b.reshape(1,-1)\n",
    "    #같은 자리의 값이 서로 다른 것들의 합\n",
    "    distance = (a!=b).sum()\n",
    "    return distance\n",
    "\n",
    "#권총영상의 해시 구하기\n",
    "query_hash = img2hash(img)\n",
    "\n",
    "#이미지 데이터셋 디렉터리의 모든 영상 파일 경로\n",
    "img_path = glob.glob(search_dir+'/**/*.jpg')\n",
    "for path in img_path:\n",
    "    #데이터셋 영상 한 개를 읽어서 표시\n",
    "    img =cv2.imread(path)\n",
    "    cv2.imshow('searching...', img)\n",
    "    cv2.waitKey() #5\n",
    "    #데이터셋 영상 한개의 해시\n",
    "    a_hash = img2hash(img)\n",
    "    #해밍거리 산출\n",
    "    dst = hamming_distance(query_hash, a_hash)\n",
    "    if dst/256 < 0.25: #해밍거리 25%이내만 출력\n",
    "        print(path, dst/256)\n",
    "        cv2.imshow(path, img)\n",
    "        \n",
    "cv2.destroyWindow('searching...') \n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abeba86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "권총 이미지를 잘 검출하긴 했으나 그외 사람얼굴이나 다른 상관없는 이미지까지 검출했음\n",
    "실무에 사용할 만큼 충분히 좋은 결과를 가져오지는 못함.\n",
    "\n",
    "현 예제는 두 해시값의 해밍거리로 영상간의 특징을 비교해서 유사도를 측정했는데 \n",
    "이 과정을 매칭이라함. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd31ba25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "앞서 한 과정들을 특징검출이라 하는데\n",
    "이 예제에서는 영상 전체의 특징ㅇ르 하나의 숫자로 변환했지만,\n",
    "회전, 크기, 방향 등에 영향이 없으면서 정확도를 높이려면 특징을 잘 나타내는 여러개의 지점을 찾아 \n",
    "그 특징을 잘 표현하고 서술할 수 있는 여러개의 숫자들로 변환해야 하는데 이를 키포인트와 특징 디스크립터라 함.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760ad38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#템플릿 매칭\n",
    "\"\"\"\n",
    "어떤 물체가 있는 영상을 준비해 두고 그 물체가 포함 되어 있을 것이라고 예상할 수 있는\n",
    "입력영상과 비교해서 물체가 매칭되는 위치를 찾는것.\n",
    "\n",
    "미리 준비해둔 영상을 템플릿 영상이라함.\n",
    "이것을 입력영상에서 찾는 것이므로 템플릿 영상은 입력영상보다 그 크기가 항상 작아야 함.\n",
    "\n",
    "cv2.matchTemplate(img, templ, method[, result, mask])\n",
    "  - img:입력영상\n",
    "  - templ:템플릿 영상\n",
    "  - method:매칭 메서드\n",
    "    -cv2.TM_SQDIFF:제곱차이매칭,완벽매칭:0 /나쁜매칭:큰값\n",
    "    -cv2.TM_SQDIFF_NORMED:제곱 차이 매칭의 정규화\n",
    "    -cv2.TM_CCOEFF:상관관계 매칭,완벽 매칭:큰값/ 나쁜매칭:0\n",
    "    -cv2.TM_CCOEFF_NORMED:상관계수 매칭의 정규화\n",
    "  -result:매칭결과, (W-w+1) x (H-h+1) 크기의 2차원배열\n",
    "    -W,H:img의 열과 행\n",
    "    -w,h:tmpl의 열과 행\n",
    "  -mask:TM_SQDIFF, TM_CCORR_NORMED인 경우 사용할 마스크\n",
    "\n",
    "minVal, maxVal, minLoc, maxLoc = cv2.minMaxLoc(src[, mask])\n",
    "  -src:입력 1채널 배열\n",
    "  -minVal, maxVal:배열 전체에서 최소값, 최대값\n",
    "  -minLoc, maxLoc:최소값과 최대값의 좌표(x,y)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "cv2.matchTemplate함수는 입력 영상 img에서 templ 인자의 영상을 슬라이딩하면서 주어진 메서드에 따라 매칭 수행\n",
    "이 함수는 유클리드 거리를 변형한 형태의 제곱차, 상관관계, 상광ㄴ계수 매칭이라는 세가지 메서드와\n",
    "각각의 메서드를 정규화한 세가지 메서드 총 여섯가지 메서드 제공\n",
    "\n",
    "cv2.TM_SQDIFF는 0인 경우에 완벽매칭을 뜻하고 값이 커질수록 나쁜매칭 의미\n",
    "cv2.TM_CCORR는 0인 경우에 나쁜매칭을 뜻하고 값이 커질수록 좋은매칭 의미\n",
    "cv2.TM_CCOEFF는 완벽 매칭은1, 나쁜매칭은 -1, 연관관계가 없는 경우 0의 값을 가짐.\n",
    "\n",
    "세가지 매칭 메서드에 각각 정규화 계수를 나눈 함수들\n",
    "cv2.TM_SQDIFF_NORMED, cv2.TM_CCORR_NORMED, cv2.TM_CCOEFF_NORMED\n",
    "이 함수의 결과는 img의 크기에서 templ의 크기를 뺀 것에 1만큼 큰 2차원 배열을 result로 얻는데,\n",
    "이 배열의 최대, 최소 값을 구하면 원하는 최선의 매칭값과 매칭점을 찾을 수 있음\n",
    "이것을 도와주는 함수가 cv2.minMaxLoc 함수임. 이 함수는 최대값과 최소값은 물론 촤소값과 최대값의 (x,y)좌표를 각각 반환\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baae1944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv2.TM_CCOEFF_NORMED -0.1780252307653427 0.5131933093070984 (42, 0) (208, 43)\n",
      "cv2.TM_CCORR_NORMED 0.827332615852356 0.9238022565841675 (85, 6) (208, 43)\n",
      "cv2.TM_SQDIFF_NORMED 0.17028295993804932 0.36860838532447815 (208, 43) (86, 7)\n"
     ]
    }
   ],
   "source": [
    "#템플릿 매칭으로 객체 위치 검출\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#입력 이미지와 템플릿 이미지 읽기\n",
    "img = cv2.imread('./img/figures.jpg')\n",
    "template = cv2.imread('./img/taekwonv1.jpg')\n",
    "th, tw = template.shape[:2]\n",
    "cv2.imshow('template', template)\n",
    "\n",
    "#세가지 매칭 메서드 순회\n",
    "methods = ['cv2.TM_CCOEFF_NORMED', 'cv2.TM_CCORR_NORMED', 'cv2.TM_SQDIFF_NORMED']\n",
    "\n",
    "for i, method_name in enumerate(methods):\n",
    "    img_draw = img.copy()\n",
    "    method = eval(method_name)\n",
    "    #템플릿매칭\n",
    "    res = cv2.matchTemplate(img, template, method)\n",
    "    #최대, 최소값과 그 좌표 구하기\n",
    "    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res)\n",
    "    print(method_name, min_val, max_val, min_loc, max_loc)\n",
    "    \n",
    "    #TM_SQDIFF의 경우 최소 값이 좋은 매칭, 나머지는 그 반드\n",
    "    if method in [cv2.TM_SQDIFF, cv2.TM_SQDIFF_NORMED]:\n",
    "        top_left = min_loc\n",
    "        match_val = min_val\n",
    "    else:\n",
    "        top_left = max_loc\n",
    "        match_val = min_val\n",
    "    #매칭 좌표를 구해서 사각형표시\n",
    "    bottom_right = (top_left[0] +tw, top_left[1] +th)\n",
    "    cv2.rectangle(img_draw, top_left, bottom_right, (0,0,255), 2)\n",
    "    #매칭 포인트 표시\n",
    "    cv2.putText(img_draw, str(match_val), top_left, cv2.FONT_HERSHEY_PLAIN, 2, (0,255,0), 1, cv2.LINE_AA)\n",
    "    cv2.imshow(method_name, img_draw)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c5dd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#영상의 특징과 키 포인트\n",
    "#코너 특징 검출\n",
    "\"\"\"\n",
    "해리스 코너 검출: 소벨 미분으로 엣지를 검출하면서 엣지의 경사도 변화량을 측정하여 변화량이 X축과 Y축 모든 방향으로 \n",
    "크게 변화하는 것을 코너로 판단.\n",
    "\n",
    "cv2.cornerHarris(src, blockSize, ksize, k[, dst, borderType])\n",
    "  -src:입력 영상, 그레이 스케일\n",
    "  -blocksize:이웃 픽셀 범위\n",
    "  -ksize:소벨 미분 커널 크기\n",
    "  -k:코너 검출 상수, 경험적 상수(0.04~0.06)\n",
    "  -dst:코너 검출 결과\n",
    "    -src와 같은 크기의 1채널 배열, 변화량의 값, 지역 최대 값이 코너점을 의미\n",
    "  -borderType:외곽영역 보정 형식\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90a7a8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#해리스 코너 검출\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('./img/house.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#해리스 코너 검출\n",
    "corner = cv2.cornerHarris(gray, 2, 3,  0.04)\n",
    "#변화량 결과의 최대 값 10% 이상의 좌표 구하기\n",
    "coord = np.where(corner > 0.1*corner.max())\n",
    "coord = np.stack((coord[1], coord[0]), axis=-1)\n",
    "\n",
    "#코너 좌표에 동그라미 그리기\n",
    "for x,y in coord:\n",
    "    cv2.circle(img, (x,y), 5, (0,0,255), 1, cv2.LINE_AA)\n",
    "    \n",
    "#변화량을 영상으로 표현하기 위해서 0-255로 정규화\n",
    "corner_norm = cv2.normalize(corner, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
    "#화면에 출력\n",
    "corner_norm = cv2.cvtColor(corner_norm, cv2.COLOR_GRAY2BGR)\n",
    "merged = np.hstack((corner_norm, img))\n",
    "cv2.imshow('Harris Corner', merged)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147306c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "cv2.cornerHarris함수의 결과는 입력 영상과 같은 크기의 1차원 배열로 지역 최대 값이 코너를 의미\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34355401",
   "metadata": {},
   "outputs": [],
   "source": [
    "#시와 토마시의 해리스 코너 검출 개선 알고리즘\n",
    "\"\"\"\n",
    "이 방법으로 검출한 코너는 객체 추적에 좋은 특징이 됨\n",
    "cv2.goodFeaturesToTrack(img, maxCorners, qualityLevel, mindistance[, corners, mask, blockSize, useHarrisDetector, k])\n",
    "  -img:입력 영상\n",
    "  -maxCorners:얻고싶은 코너 개수, 강한것 순\n",
    "  -qualityLevel:코너 간 최소거리\n",
    "  -mask:검출에 제외할 마스크\n",
    "  -blockSize = 3:코너 주변 영역의 크기\n",
    "  -useHarrisDetector=False:코너 검출 방법 선택\n",
    "    -True:해리스 코너 검출 방법, False=시와 토마시 검출 방법\n",
    "    -k:해리스 코너 검출 방법에 사용할 k 계수\n",
    "    -corners:코너 검출 좌표 결과, N x 1 x 2 크기의 배열, 실수 값이므로 정수로 변형필요\n",
    "\n",
    "useHarrisDetector인자에 True 값을 전달하면 해리스 코너 검출 방법을 사용하고 그렇지 않으면 시-토마시의 코너 검출 방법사용\n",
    "결과 값은 검출된 코너의 좌표 개수만큼 1x2 크기로 구성되어 있어서 코너의 좌표를 알기 쉬움.\n",
    "다만 좌표 값이 실수값으로 되어 있으므로 픽셀 좌표로 쓰려면 정수형으로 변환해야함.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f02f6aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#시-코마시 코너 검출\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('./img/house.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#시-토마스 코너 검출 메서드\n",
    "corners = cv2.goodFeaturesToTrack(gray, 80, 0.01, 10)\n",
    "#실수 좌표를 정수 좌표로 변환\n",
    "corners = np.int32(corners)\n",
    "\n",
    "#좌표에 동그라미 표시\n",
    "for corner in corners:\n",
    "    x,y = corner[0]\n",
    "    cv2.circle(img, (x,y), 5, (0,0,255), 1, cv2.LINE_AA)\n",
    "\n",
    "cv2.imshow('corners', img)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd5e0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#키 포인트와 특징 검출기\n",
    "\"\"\"\n",
    "opencv는 여러 특징점 검출 알고리즘 중 어떤 것을 사용하든 간에 동일한 코드로 특징점을 검출할 수 있게\n",
    "각 알고리즘 구현 클래스가 추상 클래스를 상속받는 방법으로 인터페이스를 통일\n",
    "\n",
    "opencv는 모든 특징 검출기를(12가지) cv2.Feature2D 클래스를 상속받아 구현했으며, \n",
    "이것으로부터 추출된 특징점은 cv2.keyPoint라는 객체에 담아 표현\n",
    "\n",
    "cv2.Feature2D를 상속받은 특징 검출기는 detect함수를 구현하고 있고 \n",
    "이 함수는 특징점의 좌표와 추가 정보를 담은 cv2.KeyPoint 객체를 리스트에 담아 반환\n",
    "\n",
    "Keypoints = detector.detect(img [, mask]): 키 포인트 검출 함수\n",
    "  -img:입력 영상, 바이너리 스케일\n",
    "  -mask:검출 제외 마스크\n",
    "  -keypoints:특징점 검출 결과, KeyPoint의 리스트\n",
    "\n",
    "KeyPoint:특징점 정보를 담는 객체\n",
    "  -pt:키포인트(x,y)좌표, float타입으로 정수로 변환 필요\n",
    "  -size:의미 있는 키 포인트 이웃의 반지름\n",
    "  -angle:특징점 방향(시계방향, -1=의미 없음)\n",
    "  -response:특징점 반응 강도(추출기에 따라 다름)\n",
    "  -octave:발견된 이미지 피라미드 계층\n",
    "  -class_id:키 포인트가 속한 객체 ID\n",
    "  \n",
    "키 포인트의 속성 중 특징점의 좌표 정보인 pt속성은 항상 값을 갖지만 나머지 속성은 사용하는 검출기에 따라 값이 없을 수도 있음\n",
    "검출한 키 포인트를 영상에 표시하고 싶을 때는 cv2.circle함수로 pt의 좌표와 size값을 표시할 수도 있음.\n",
    "하지만 opencv는 키 포인트를 영상에 표시해주는 전용 함수를 제공\n",
    "\n",
    "cv2.drawKeypoints(img, keypoints, outImg[, color[, flags]])\n",
    "  -img:입력이미지\n",
    "  -keypoints:표시할 키 포인트 리스트\n",
    "  -outImg:키 포인트가 그려진 결과 이미지\n",
    "  -color:표시할 색상(기본값: 랜덤)\n",
    "  -flags:표시 방법 선택 플래그\n",
    "    -cv2.DRAW_MATCHES_FLAGS_DEFAULT:좌표 중심에 동그라미만 그림(기본값)\n",
    "    -cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS:동그라미의 크기를 size와 angle을 반영해서 그림\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37be23b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GFTTDetector\n",
    "\"\"\"\n",
    "cv2.goodFeaturesToTrack함수로 구현된 특징 검출기\n",
    "cv2.GFTTDetector(img, maxCorners, qualityLevel, mindistance[, corners, mask, blockSize, useHarrisDetector, k])\n",
    "인자는 cv2.goodFeaturesToTrack과 동일\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8907b3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GFTTDetector로 키 포인트 검출\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('./img/house.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#Good feature to trac 검출기 생성\n",
    "#함수 디폴트로 시-토마시 알고리즘 사용하는 검출기 생성\n",
    "gftt = cv2.GFTTDetector_create()\n",
    "#키 포인트 검출\n",
    "keypoints = gftt.detect(gray, None)\n",
    "#키 포인트 그리기\n",
    "img_draw = cv2.drawKeypoints(img, keypoints, None)\n",
    "\n",
    "#결과출력\n",
    "cv2.imshow('GFTTDetector', img_draw)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9f18eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FAST\n",
    "\"\"\"\n",
    "속도를 개선한 알고리즘으로\n",
    "코너를 검출할 때 미분연산으로 엣지검출을 하지 않고 픽셀을 중심으로 특정 개수의 픽셀로 원을 그려서 그 안의 픽셀들이\n",
    "중심 픽셀 값보다 임계값 이상 밝거나 어두운 것이 특정 개수 이상 연속되면 코너로 판단\n",
    "\n",
    "cv2.FastFeatureDetector_create([threshold[, nonmaxSupperssion, type]]) ???\n",
    "  -threshold = 10:코너 판단 임계값\n",
    "  -nonmaxSuppression = True:최대 점수가 아닌 코너 억제\n",
    "  -type:엣지 검출 패턴\n",
    "    -cv2.FastFeatureDetector_TYPE_9_6:16개중 9개 연속(기본값)\n",
    "    -cv2.FastFeatureDetector_TYPE_7_12:12개중 7개 연속\n",
    "    -cv2.FastFeatureDetector_TYPE_5_8:8개중 5개 연속\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dc79925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FAST로 키 포인트 검출\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('./img/house.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#Good feature to trac 검출기 생성\n",
    "#함수 디폴트로 시-토마시 알고리즘 사용하는 검출기 생성\n",
    "fast = cv2.FastFeatureDetector_create(50)\n",
    "#키 포인트 검출\n",
    "keypoints = fast.detect(gray, None)\n",
    "#키 포인트 그리기\n",
    "img = cv2.drawKeypoints(img, keypoints, None)\n",
    "\n",
    "#결과출력\n",
    "cv2.imshow('FAST', img)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5c552f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SimpleBlobDetector\n",
    "\"\"\"\n",
    "BLOB(Binary Large Object)는 바이너리 스케일 이미지의 연결된 픽셀 그룹을 말하는 것\n",
    "자잘한 객체는 노이즈로 판단하고 특정 크기 이상의 큰 객체에만 관심을 두는 방법\n",
    "BLOB는 코너를 이용한 특징 검출과는 방식이 다르지만 영상의 특징을 표현하기 좋은 또 하나의 방법임.\n",
    "\n",
    "cv2.SimpleBlobDetector_create([parameters]):BLOB 검출기 생성자\n",
    "  -parameters:BLOB 검출 필터 인자 객체\n",
    "cv2.SimpleBlobDetector_Params()\n",
    "  -minThreshold, , max_Threshold, thresholdStep: BLOB를 생성하기 위한 경계값\n",
    "  (minThreshold에서 maxThreshold를 넘지 않을 때까지 thresholdStep만큼 증가)\n",
    "  -minRepeatability:BLOB에 참여하기 위한 연속된 경계값의 개수\n",
    "  -minDistBetweenBlobs:두 BLOB를 하나의 BLOB로 간주한 거리\n",
    "  -filterByArea:면적 필터 옵션\n",
    "  -minArea, maxArea:min~max 범위의 면적만 BLOB 검출\n",
    "  -filterbyCircularity:원형 비율 필터 옵션\n",
    "  -minCircularity, maxCircularity:min~max 범위의 원형 비율만 BLOB로 검출\n",
    "  -filterByColor:밝기를 이용한 필터 옵션\n",
    "  -blobColor:0 = 검은색 BLOB 검출, 255=흰색BLOB 검출\n",
    "  -filterByConvexity:볼록 비율 필터 옵션\n",
    "  -minConvexity, maxConvexity:min~max 범위의 볼록 비율만 BLOB로 검출\n",
    "  -filterByInertia:관성비율 필터 옵션\n",
    "  -minInertiaRatio, maxInertiaRatio:min~max 범위의 관성 비율만 BLOB로 검출\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c33e3e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SimpleBlobDetector검출기\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('./img/house.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#SimpleBlobDetector검출기 생성\n",
    "detector = cv2.SimpleBlobDetector_create()\n",
    "#키 포인트 검출\n",
    "keypoints = detector.detect(gray, None)\n",
    "#키 포인트 그리기\n",
    "img = cv2.drawKeypoints(img, keypoints, None, (0,0,255), flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "#결과출력\n",
    "cv2.imshow('Blob', img)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a035c036",
   "metadata": {},
   "outputs": [],
   "source": [
    "#필터 옵션으로 생성한 SimpleBlobDetector 검출기\n",
    "\n",
    "\"\"\"\n",
    "SimpleBlobDetector 객체를 필터 옵션 객체를 이용해서 생성.\n",
    "파라미터는 경계값을 넓게 적용하고, 원형, 볼록, 색상 필터를 모두 끄고 검출하여 이전에 기본으로 생성한 검출기보다\n",
    "많은 BLOB를 검출함. 필요시 필터값을 조정해서 원하는 모양과 크기의 BLOB 검출 가능\n",
    "\"\"\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('./img/house.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#BLOB 검출 파라미터 생성\n",
    "params = cv2.SimpleBlobDetector_Params()\n",
    "\n",
    "#경계값 조정\n",
    "params.minThreshold = 10\n",
    "params.maxThreshold= 240\n",
    "params.thresholdStep = 5\n",
    "\n",
    "#면적 필터를 켜고 최소 값 지정\n",
    "params.filterByArea = True\n",
    "params.minArea = 200\n",
    "\n",
    "#컬러, 볼록비율, 원형비율 필터 옵션 끄기\n",
    "params.filterByColor = False\n",
    "params.filterByConvexity = False\n",
    "params.filterByInertia= False\n",
    "params.filterByCircularity=False\n",
    "\n",
    "#필터 파라미터로 BLOB 검출기 생성\n",
    "detector = cv2.SimpleBlobDetector_create(params)\n",
    "#키 포인트 검출\n",
    "keypoints = detector.detect(gray)\n",
    "#키 포인트 그리기\n",
    "img = cv2.drawKeypoints(img, keypoints, None, None, cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "#결과출력\n",
    "cv2.imshow('Blob with params', img)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "329c05a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##디스크립터 추출기\n",
    "#특징 디스크립터와 추출기\n",
    "\"\"\"\n",
    "키 포인트는 영상의 특징이 있는 픽셀의 좌표와 그 주변 픽셀과의 관계에 대한 정보를 가짐.\n",
    "그 중 가장 대표적인 것이 size와 angle 속성으로, 코너 특징인 경우 엣지의 경사도 규모와 방향을 나타냄.\n",
    "특징을 나타내는 값을 매칭에 사용하기 위해서는 회전, 크기, 방향등에 영향이 없어야 하는데, 이를 위해 특징 디스크립터가 필요.\n",
    "\n",
    "특징 디스크립터는 키 포인트 주변 픽셀을 일정한 크기의 블록으로 나누어 각 블록에 속한 픽셀의 그레이디언트 히스토그램을 계산 한 것\n",
    "키포인트 주위의 밝기, 색상, 방향, 크기 등의 정보를 표현한 것. 특징 디스크립터는 추출하는 알고리즘에 따라 내용, 모양, 그리고 크기가\n",
    "각각 다를 수 있지만, 일반적으로는 키 포인트에 적용하는 주변 블록의 크기에 8방향의 경사도를 표현하는 형태인 경우가 많음\n",
    "ex)4x4크기의 블록인 경우 한개의 키 포인트 당 4x4x8=128개의 값으로 구성\n",
    "\n",
    "opencv는 특징 디스크립터를 추출하기 위한 방법으로 통일된 인터페이스를 제공하기 위해 특징 검출기와 같은 \n",
    "cv2.Feature2D 클래스를 상속받아 구현했고 이때 사용하는 함수는 아래의 두 함수\n",
    "\n",
    "keypoints, descripts = detector.compute(image, keypoints[, descriptors])\n",
    ":키 포인트를 전달하면 특징 디스크립터를 계산해서 반환\n",
    "\n",
    "keypoints, descriptors =detector.detectAndCompute(image, mask[, descriptors, useProvidedKeypoints])\n",
    ": 키 포인트 검출과 특징 디스크립터 계산을 한번에 수행\n",
    "  -image:입력 영상\n",
    "  -keypoints:디스크립터 계산을 위해 사용할 키 포인트\n",
    "  -descriptors:계산된 디스크립터\n",
    "  -mask:키 포인트 검출에 사용할 마스크\n",
    "  -useProvidedKeypoints:True인 경우 키 포인트 검출을 수행하지 않음(사용안함)\n",
    "\n",
    "cv2.Feature2D를 상속받은 몇몇 특징 검출기는 detect함수만 구현되어 있고 compute와 detectAndcompute 함수는 구현되어 있지 않은 경우도\n",
    "있고 그 반대의 경우도 있음\n",
    "GFTTDetector와 SimpleBlobDetector는 compute와 detectAndCompute가 구현되어 있지 않음\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed645e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SIFT \n",
    "\"\"\"\n",
    "이미지 피라미드를 이용해서 크기변화에 따른 특징 검출의 문제를 해결한 알고리즘.\n",
    "이 알고리즘은 특허권이 있어 상업적 사용에는 제약이 있으며, Opencv는 엑스트라(contrib)모듈에만 포함됨\n",
    "\n",
    "cv2.xfeatures2d.SIFT_create([, nfeatures[, n0ctaveLayers[, contrastThreshold[, edgeThreshold[, sigma]]]]])\n",
    "  -nfeatures:검출 최대 특징 수\n",
    "  -n0ctaveLayers:이미지 피라미드에 사용할 계층 수\n",
    "  -contrastThreshold:필터링할 빈약한 특징 문턱 값\n",
    "  -edgeThreshold:필터링할 엣지 문턱 값\n",
    "  -sigma:이미지 피라미드 0계층에서 사용할 가우시안 필터의 시그마 값\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4eda75c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoint: 413 descriptor: (413, 128)\n",
      "[[  1.   1.   1. ...   0.   0.   1.]\n",
      " [  8.  24.   0. ...   1.   0.   4.]\n",
      " [  0.   0.   0. ...   0.   0.   2.]\n",
      " ...\n",
      " [  1.   8.  71. ...  73. 127.   3.]\n",
      " [ 35.   2.   7. ...   0.   0.   9.]\n",
      " [ 36.  34.   3. ...   0.   0.   1.]]\n"
     ]
    }
   ],
   "source": [
    "#SHIFT로 키 포인트 및 디스크립터 추출\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('./img/house.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#SHIFT 추출기 생성\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "#키포인트 검출과 서술자 계산\n",
    "keypoints, descriptor = sift.detectAndCompute(gray, None)\n",
    "print('keypoint:', len(keypoints), 'descriptor:', descriptor.shape)\n",
    "print(descriptor)\n",
    "\n",
    "#키 포인트 그리기\n",
    "img = cv2.drawKeypoints(img, keypoints, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "#결과출력\n",
    "cv2.imshow('SIFT', img)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb4ff98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SURF\n",
    "\"\"\"\n",
    "SIFT는 크기 변화에 따른 특징 검출 문제를 해결하기 위해 이미지 피라미드를 사용하므로 속도가 느림\n",
    "SURF는 이미지 피라미드 대신 필터의 커널 크기를 바꾸는 방식으로 성능을 개선한 알고리즘\n",
    "\n",
    "cv2.xfeatures2d.SURF_crate([hessianThreshold, n0ctaves, n0ctaveLayers, extended, upright])\n",
    "  -hessianThreshold:특징 추출 경계값(100)\n",
    "  -n0ctaves:이미지 피라미드 계층 수(3)\n",
    "  -extended:디스크립터 생성 플래그(False), True:128개, False:64개\n",
    "  -upright:방향 계산 플래그(False), True:방향무시, False:방향 적용\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1627c376",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Argument 'hessianThreshold' can not be treated as a double",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16492/1246921535.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#SURF 추출기 생성(경계: 1000, 피라미드:3, 서술자확장:True, 방향적용:True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0msurf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxfeatures2d\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSURF_create\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mkeypoints\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msurf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectAndCompute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Argument 'hessianThreshold' can not be treated as a double"
     ]
    }
   ],
   "source": [
    "#SURF로 키 포인트 및 디스크립터 추출\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('./img/house.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#SURF 추출기 생성(경계: 1000, 피라미드:3, 서술자확장:True, 방향적용:True)\n",
    "surf = cv2.xfeatures2d.SURF_create(1000, 3, True, True)\n",
    "\n",
    "keypoints, desc = surf.detectAndCompute(gray, None)\n",
    "print('desc.shape:', desc.shape, 'desc:', desc)\n",
    "\n",
    "#키 포인트 그리기\n",
    "img_draw = cv2.drawKeypoints(img, keypoints, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "#결과출력\n",
    "cv2.imshow('SURF', img_draw)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "afe2fed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ORB pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46db676",
   "metadata": {},
   "outputs": [],
   "source": [
    "#특징 매칭\n",
    "\"\"\"\n",
    "특징 매칭이란 서로 다른 두 영상에서 구한 키 포인트와 특징 디스크립터들을 각각 비교해서 그 \n",
    "거리가 비슷한 것끼리 작짓는 것을 말함. \n",
    "짝지어진 특징점들 중에 거리가 유의미한 것들을 모아서 대칭점으로 표시하면 그 개수에 따라 두 영상이\n",
    "얼마나 비슷한지 측정할 수 있고 충분히 비슷한 영상이라면 비슷한 모양의 영역을 찾아낼 수도 있ㅇ,ㅁ.\n",
    "특징 매칭은 파노라마 사진 생성, 이미지 검색, 드록한 객체인식 등 다양하게 응용가능\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302206fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#특징 매칭 인터페이스\n",
    "\"\"\"\n",
    "opencv는 특정 매칭에 사용하는 여러가지 알고리즘을 통일된 인터페이스로 제공하기 위해\n",
    "모든 특징 매칭 알고리즘을 추상 클래스인 cv2.DescriptorMatcher를 상속받아 구현\n",
    "그리고 매칭 결과의 구조와 모양을 통일하기 위해 모든 매칭 결과는 cv2.DMatch객체에 담아 반환\n",
    "\n",
    "match=cv2.DescriptorMatcher_create(matcherType):매칭기 생성자\n",
    "  -matcherType:생성할 구현 클래스의 알고리즘, 문자열\n",
    "    -\"BruteForce\": NORM_L2를 사용하는 BFMatcher\n",
    "    -\"BruteForce-L1\": NORM_L1을 사용하는 BFMatcher\n",
    "    -\"BruteForce-Hamming\": NORM_HAMMING을 사용하는 BFMatcher\n",
    "    -\"BruteForce-Hamming(2)\": NORM_HAMMING2을 사용하는 BFMatcher\n",
    "    -\"FlannBased\": NORM_L2를 사용하는 FlannBasedMatcher\n",
    "\n",
    "matches = matcher.match(queryDescriptors, trainDescriptors[, mask]):1개의 최적 매칭\n",
    "  -queryDescriptors:특징 디스크립터 배열, 매칭의 기준이 될 디스크립터\n",
    "  -trainDescriptors:특징 디스크립터 배열, 매칭의 대상이 될 디스크립터\n",
    "  -mask:매칭 진행 여부 마스크\n",
    "  -matches:매칭 결과, DMatch 객체의 리스트\n",
    "\n",
    "matches=matcher.knnmatch(queryDescriptors, trainDescriptors, k[, mask[, compactResult]]):k개의 가장 근접한 매칭\n",
    "  -k:매칭할 근접 이웃 개수\n",
    "  -compactResult =False :True:매칭이 없는 경우 매칭 결과에 불포함\n",
    "\n",
    "matches=matcher.radiusmatch(queryDescriptors, trainDescriptors, maxDistance[, mask[, compactResult]]):maxDistance 이내의 거리 매칭\n",
    "  -maxDistance: 매칭 대상 거리\n",
    "\n",
    "DMatch:매칭 결과를 표현하는 객체\n",
    "  -queryIdx:queryDescriptor의 인덱스\n",
    "  -trainIdx:trainDescriptor의 인덱스\n",
    "  -imgIdx:trainDescriptor의 이미지 인덱스\n",
    "  -distance:유사도 거리\n",
    "\n",
    "cv2.drawMatches(img1, kp1, img2, kp2, matches, flags):매칭점을 영상에 표시\n",
    "  -img1, kp1:queryDescirptor의 영상과 키 포인트\n",
    "  -img2, kp2:trainDescriptor의 이미지 인덱스\n",
    "  -matches:매칭 결과\n",
    "  -flags:매칭점 그리기 옵션\n",
    "    -cv2.DRAW_MATCHES_FLAGS_DEFAULT:결과 이미지 새로 생성(기본값)\n",
    "    -cv2.DRAW_MATCHES_FLAGS_DRAW_OVER_OUTIMG:결과 이미지 새로 생성 안함\n",
    "    -cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS_:키 포인트 크기와 방향도 그리기\n",
    "    -cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS:한쪽만 있는 매칭 결과 그리기 제외\n",
    "    \n",
    "331-332    \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b9b027",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BFMatcher\n",
    "\"\"\"\n",
    "Brute_force 매칭기는 queryDescriptor와 trainDescriptor를 일일이 전수 조사해서\n",
    "매칭하는 알고리즘으로 opencv는 cv2.BFMatcher를 클래스로 제공\n",
    "\n",
    "이 클래스에서 객체를 생성하려면 추상 클래스의 생성 함수인 \n",
    "cv2.DescriptorMatcher_create도 있지만 구현 클래스에서 직접 생성할 수도 있음.\n",
    "\n",
    "cv2.BFMatcher_create([normType[, crossCheck]])\n",
    "  -normType:거리측정 알고리즘\n",
    "    -cv2.NORM_L1\n",
    "    -cv2.NORM_L2\n",
    "    -cv2.NORM_L1SQR\n",
    "    -cv2.NORM_HAMMING\n",
    "    -cv2.NORM_HAMMING2\n",
    "  -crossCheck=False:상호매칭이 있는 것만 반영\n",
    "  \n",
    "거리측정 알고리즘은 세가지 유클리드 거리와 두가지 해밍거리 중에서 선택가능\n",
    "SIFT와 SURF로 추출한 디스크립터의 경우 NORM_L1,NORM_L2가 적합하고\n",
    "ORB는 NORM_HAMMING가 적합하며, NORM_HAMMING2는 ORB의 WTA_K가 3이나 4인 경우에 적합\n",
    "crossCheck를 True로 설정하면 양쪽디스크립터 모두에게서 매칭이 완성된 것만 반영하므로\n",
    "불필요한 매칭을 줄일수 있지만 그만큼 속도가 느려짐\n",
    "\"\"\"\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "249a2915",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BFMatcher와 SIFT로 매칭\n",
    "import cv2, numpy as np\n",
    "\n",
    "img1 = cv2.imread('./img/taekwonv1.jpg')\n",
    "img2 = cv2.imread('./img/figures.jpg')\n",
    "gray1 =  cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 =  cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#SHIFT 디스크립터 추출기 생성\n",
    "detector = cv2.xfeatures2d.SIFT_create()\n",
    "#각 영상에 대해 키 포인트와 디스크립터 추출\n",
    "kp1, desc1 = detector.detectAndCompute(gray1, None)\n",
    "kp2, desc2 = detector.detectAndCompute(gray2, None)\n",
    "\n",
    "#BFMatcher 생성, L1거리, 상호 체크\n",
    "matcher = cv2.BFMatcher(cv2.NORM_L1, crossCheck=True)\n",
    "#매칭 계산\n",
    "matches = matcher.match(desc1, desc2)\n",
    "#매칭 결과 그리기\n",
    "res = cv2.drawMatches(img1, kp1, img2, kp2, matches, None, flags = cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "#결과 출력\n",
    "cv2.imshow('BFMatcher + SIFT', res)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bed32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BFMatcher와 SURF로 매칭\n",
    "import cv2, numpy as np\n",
    "\n",
    "img1 = cv2.imread('./img/taekwonv1.jpg')\n",
    "img2 = cv2.imread('./img/figures.jpg')\n",
    "gray1 =  cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 =  cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#SHIFT 디스크립터 추출기 생성\n",
    "detector = cv2.xfeatures2d.SIFT_create()\n",
    "#각 영상에 대해 키 포인트와 디스크립터 추출\n",
    "kp1, desc1 = detector.detectAndCompute(gray1, None)\n",
    "kp2, desc2 = detector.detectAndCompute(gray2, None)\n",
    "\n",
    "#BFMatcher 생성, L1거리, 상호 체크\n",
    "matcher = cv2.BFMatcher(cv2.NORM_L1, crossCheck=True)\n",
    "#매칭 계산\n",
    "matches = matcher.match(desc1, desc2)\n",
    "#매칭 결과 그리기\n",
    "res = cv2.drawMatches(img1, kp1, img2, kp2, matches, None, flags = cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "#결과 출력\n",
    "cv2.imshow('BFMatcher + SIFT', res)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc21946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#중간 pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d0edde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#좋은 매칭점 찾기\n",
    "\n",
    "\"\"\"\n",
    "위에서 본 코드는 잘된 매칭정보 뿐 아니라 잘못된 매칭정보까지 너무 많이 포함했음\n",
    "그래서 매칭 결과에서 쓸모 없는 매칭점은 버리고 좋은 매칭점만을 골라내는 작업 필요\n",
    "만약 좋은 매칭점만 골라냈더더니 남아있는 매칭점이 몇개 없다면 두 영상간에 연관관계가 없다고 판단\n",
    "\n",
    "cv2.DescriptorMatcher 추상 클래스에는 매칭을 위한 세가지 함수가 있는데, 그중에 지정된 거리 이내의 매칭점만을\n",
    "반환하는 radiusMatch 함수는 maxDistance 값을 조정하는 것 말고는 의미가 없으므로 제외시키고, \n",
    "match와 knnmatch를 중점적으로 보도록 한다.\n",
    " \n",
    "match함수는 디스크립터 하나당 한개의 매칭을 반환하므로 매칭점마다 어떤 것은 거리값이\n",
    "작고 어떤것은 크고 하는 식으로 뒤섞여 있음. 이때에는 가장 작은 거리 값과 큰 거리 값의 상위 몇 \n",
    "퍼센트만을 골라서 좋은 매칭점으로 분류할 수 있음\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74c7d962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches:18/127, min:24.00, max:78.00, thresh:34.80\n"
     ]
    }
   ],
   "source": [
    "#match함수로부터 좋은 매칭점 찾기\n",
    "import cv2, numpy as np\n",
    "\n",
    "img1 = cv2.imread('./img/taekwonv1.jpg')\n",
    "img2 = cv2.imread('./img/figures.jpg')\n",
    "gray1 =  cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 =  cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#SHIFT 디스크립터 추출기 생성\n",
    "detector = cv2.ORB_create()\n",
    "#각 영상에 대해 키 포인트와 디스크립터 추출\n",
    "kp1, desc1 = detector.detectAndCompute(gray1, None)\n",
    "kp2, desc2 = detector.detectAndCompute(gray2, None)\n",
    "#DF_HAMMING으로 매칭\n",
    "matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "matches = matcher.match(desc1, desc2)\n",
    "\n",
    "#매칭 결과를 거리 기준 오름차순으로 정렬\n",
    "matches = sorted(matches, key=lambda x:x.distance)\n",
    "#최소거리값과 최대 거리값 확보\n",
    "min_dist, max_dist = matches[0].distance, matches[-1].distance\n",
    "#최소 거리의 20%지점을 임계점으로 설정\n",
    "ratio=0.2\n",
    "good_thresh = (max_dist - min_dist) * ratio + min_dist\n",
    "#임계점보다 작은 매칭점만 좋은 매칭점으로 분류\n",
    "good_matches = [m for m in matches if m.distance < good_thresh]\n",
    "print('matches:%d/%d, min:%.2f, max:%.2f, thresh:%.2f'%(len(good_matches),len(matches), min_dist, max_dist, good_thresh))\n",
    "#좋은 매칭점만 그리기\n",
    "res = cv2.drawMatches(img1, kp1, img2, kp2, good_matches, None, flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "#결과 출력\n",
    "cv2.imshow('good Match', res)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb5538e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1edf7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#카메라로 객체 매칭\n",
    "import cv2, numpy as np\n",
    "\n",
    "img1 = None \n",
    "win_name ='Camera Matching'\n",
    "WIN_MATCH = 10\n",
    "\n",
    "#ORB검출기 생성\n",
    "detector = cv2.ORB_create(1000)\n",
    "#Flann추출기 생성\n",
    "FLANN_INDEX_LSH =6\n",
    "index_params =dict(algorithm = FLANN_INDEX_LSH,\n",
    "                  table_number = 6,\n",
    "                  key_size=12,\n",
    "                  multi_probe_level=1)\n",
    "search_params=dict(checks=32)\n",
    "matcher = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "#카메라 캡쳐 연결 및 프레임 크기 축소\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if img1 is None: #등록된 이미지 없음, 매칭 과정 없이 카메라 영상을 곧바로 입력\n",
    "        res=frame\n",
    "    else: #등록된 이미지가 있는경우, 매칭 시작\n",
    "        img2=frame\n",
    "        gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "        gray1 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "        #키 포인트와 디스크립터 추출\n",
    "        kp1, desc1 = detector.detectAndCompute(gray1, None)\n",
    "        kp2, desc2 = detector.detectAndCompute(gray2, None)\n",
    "        #k=2로 knnMatch\n",
    "        matches = matcher.knnMatch(desc1, desc2, 2)\n",
    "        #이웃 거리의 75%로 좋은 매칭점 추출\n",
    "        ratio = 0.75\n",
    "        good_matches = [m[0] for m in matches \n",
    "                       if len(m) == 2 and m[0].distance < m[1].distance * ratio]\n",
    "        print('good matches:%d/%d'%(len(good_matches), len(matches)))\n",
    "        #모든 매칭점을 그리지 못하게 마스크를 0dmfh codna\n",
    "        matchesMask = np.zeros(len(good_matches)).tolist()\n",
    "        #좋은 매칭점이 최소 개수 이상인 경우\n",
    "        if len(good_matches) > MIN_MATCH:\n",
    "            #좋은 매칭점으로 원본과 대상 영상의 좌표 구하기\n",
    "            src_pts = np.float32([kp1[m.queryIdx].pt for m in good_matches])\n",
    "            dst_pts = np.float32([kp1[m.trainIdx].pt for m in good_matches])\n",
    "            #원근 변환행렬 구하기\n",
    "            mtrx, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "            accuracy=float(mask.sum()) / mask.size\n",
    "            print('accuracy: %d/%d(%.2f%%)'%(mask.sum(), mask.size, accuracy))\n",
    "            if mask.sum() > MIN_MATCH: #정상치 매칭점이 최소 개수 이상인 경우\n",
    "            #이상점 매칭점만 그리게 마스크 설정\n",
    "                matchesMask = mask.raval().tolist()\n",
    "                #원본 영상 좌표로 원근 변환 후 영역 표시\n",
    "                h,w = img1.shape[:2]\n",
    "                pts=np.float32([[0,0]], [[0,h-1]], [[w-1,h-1]], [[w-1,0]])\n",
    "                dst = cv2.perspectiveTransform(pts, mtrx)\n",
    "                img2 = cv2.polylines(img2, [np.int32(dst)], True, 255, 3, cv2.LINE_AA)\n",
    "        #마스크로 매칭점 그리기\n",
    "        res = cv2.drawMatches(img1, kp1, img2, kp2, good_matches, None, \n",
    "                             matchesMask=matchesMask,\n",
    "                             flags = cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "    #결과 출력\n",
    "    cv2.imshow(win_name, res)\n",
    "    key=cv2.waitKey(1)\n",
    "    if key == 27: #esc 종료\n",
    "        break\n",
    "    elif key == ord(' '): #스페이스바를 누르면 ROI로 img1 설정\n",
    "        x,y,w,h = cv2.selectROI(win_name, frame, False)\n",
    "        if w and h:\n",
    "            img1 = frame[y:y+h, x:x+w]\n",
    "else:\n",
    "    print(\"cnt't open camera\")\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eeafaac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good matches:18/1000\n",
      "accuracy: 6/18(0.33%)\n",
      "good matches:23/1000\n",
      "accuracy: 8/23(0.35%)\n",
      "good matches:21/1000\n",
      "accuracy: 8/21(0.38%)\n",
      "good matches:15/1000\n",
      "accuracy: 5/15(0.33%)\n",
      "good matches:21/1000\n",
      "accuracy: 6/21(0.29%)\n",
      "good matches:19/1000\n",
      "accuracy: 6/19(0.32%)\n",
      "good matches:16/1000\n",
      "accuracy: 5/16(0.31%)\n",
      "good matches:13/1000\n",
      "accuracy: 5/13(0.38%)\n",
      "good matches:22/1000\n",
      "accuracy: 9/22(0.41%)\n",
      "good matches:20/1000\n",
      "accuracy: 7/20(0.35%)\n",
      "good matches:27/1000\n",
      "accuracy: 6/27(0.22%)\n",
      "good matches:22/1000\n",
      "accuracy: 6/22(0.27%)\n",
      "good matches:20/1000\n",
      "accuracy: 6/20(0.30%)\n",
      "good matches:16/1000\n",
      "accuracy: 4/16(0.25%)\n",
      "good matches:12/1000\n",
      "accuracy: 8/12(0.67%)\n",
      "good matches:17/1000\n",
      "accuracy: 6/17(0.35%)\n",
      "good matches:12/1000\n",
      "accuracy: 4/12(0.33%)\n",
      "good matches:17/1000\n",
      "accuracy: 5/17(0.29%)\n",
      "good matches:2/1000\n",
      "good matches:8/1000\n",
      "good matches:14/1000\n",
      "accuracy: 6/14(0.43%)\n",
      "good matches:12/1000\n",
      "accuracy: 8/12(0.67%)\n",
      "good matches:9/1000\n",
      "good matches:16/1000\n",
      "accuracy: 7/16(0.44%)\n",
      "good matches:8/1000\n",
      "good matches:17/1000\n",
      "accuracy: 7/17(0.41%)\n",
      "good matches:18/1000\n",
      "accuracy: 6/18(0.33%)\n",
      "good matches:17/1000\n",
      "accuracy: 8/17(0.47%)\n",
      "good matches:27/1000\n",
      "accuracy: 11/27(0.41%)\n",
      "good matches:18/1000\n",
      "accuracy: 9/18(0.50%)\n",
      "good matches:15/1000\n",
      "accuracy: 9/15(0.60%)\n",
      "good matches:17/1000\n",
      "accuracy: 6/17(0.35%)\n",
      "good matches:24/1000\n",
      "accuracy: 9/24(0.38%)\n",
      "good matches:17/1000\n",
      "accuracy: 5/17(0.29%)\n",
      "good matches:14/1000\n",
      "accuracy: 5/14(0.36%)\n",
      "good matches:17/1000\n",
      "accuracy: 8/17(0.47%)\n",
      "good matches:20/1000\n",
      "accuracy: 14/20(0.70%)\n",
      "good matches:16/1000\n",
      "accuracy: 12/16(0.75%)\n",
      "good matches:17/1000\n",
      "accuracy: 6/17(0.35%)\n",
      "good matches:20/1000\n",
      "accuracy: 11/20(0.55%)\n",
      "good matches:15/1000\n",
      "accuracy: 11/15(0.73%)\n",
      "good matches:17/1000\n",
      "accuracy: 9/17(0.53%)\n",
      "good matches:13/1000\n",
      "accuracy: 9/13(0.69%)\n",
      "good matches:23/1000\n",
      "accuracy: 9/23(0.39%)\n",
      "good matches:16/1000\n",
      "accuracy: 9/16(0.56%)\n",
      "good matches:24/1000\n",
      "accuracy: 14/24(0.58%)\n",
      "good matches:17/1000\n",
      "accuracy: 10/17(0.59%)\n",
      "good matches:21/1000\n",
      "accuracy: 12/21(0.57%)\n",
      "good matches:13/1000\n",
      "accuracy: 6/13(0.46%)\n",
      "good matches:27/1000\n",
      "accuracy: 11/27(0.41%)\n",
      "good matches:20/1000\n",
      "accuracy: 11/20(0.55%)\n",
      "good matches:27/1000\n",
      "accuracy: 15/27(0.56%)\n",
      "good matches:32/1000\n",
      "accuracy: 15/32(0.47%)\n",
      "good matches:30/1000\n",
      "accuracy: 15/30(0.50%)\n",
      "good matches:21/1000\n",
      "accuracy: 16/21(0.76%)\n",
      "good matches:25/1000\n",
      "accuracy: 14/25(0.56%)\n",
      "good matches:25/1000\n",
      "accuracy: 10/25(0.40%)\n",
      "good matches:26/1000\n",
      "accuracy: 12/26(0.46%)\n",
      "good matches:21/1000\n",
      "accuracy: 9/21(0.43%)\n",
      "good matches:18/1000\n",
      "accuracy: 8/18(0.44%)\n",
      "good matches:21/1000\n",
      "accuracy: 13/21(0.62%)\n",
      "good matches:24/1000\n",
      "accuracy: 15/24(0.62%)\n",
      "good matches:23/1000\n",
      "accuracy: 12/23(0.52%)\n",
      "good matches:26/1000\n",
      "accuracy: 10/26(0.38%)\n",
      "good matches:26/1000\n",
      "accuracy: 13/26(0.50%)\n",
      "good matches:22/1000\n",
      "accuracy: 9/22(0.41%)\n",
      "good matches:28/1000\n",
      "accuracy: 17/28(0.61%)\n",
      "good matches:19/1000\n",
      "accuracy: 9/19(0.47%)\n",
      "good matches:21/1000\n",
      "accuracy: 15/21(0.71%)\n",
      "good matches:18/1000\n",
      "accuracy: 7/18(0.39%)\n",
      "good matches:15/1000\n",
      "accuracy: 10/15(0.67%)\n",
      "good matches:19/1000\n",
      "accuracy: 7/19(0.37%)\n",
      "good matches:29/1000\n",
      "accuracy: 9/29(0.31%)\n",
      "good matches:23/1000\n",
      "accuracy: 10/23(0.43%)\n",
      "good matches:26/1000\n",
      "accuracy: 13/26(0.50%)\n",
      "good matches:23/1000\n",
      "accuracy: 13/23(0.57%)\n",
      "good matches:17/1000\n",
      "accuracy: 11/17(0.65%)\n",
      "good matches:15/1000\n",
      "accuracy: 9/15(0.60%)\n",
      "good matches:24/1000\n",
      "accuracy: 13/24(0.54%)\n",
      "good matches:27/1000\n",
      "accuracy: 15/27(0.56%)\n",
      "good matches:24/1000\n",
      "accuracy: 9/24(0.38%)\n",
      "good matches:22/1000\n",
      "accuracy: 11/22(0.50%)\n",
      "good matches:26/1000\n",
      "accuracy: 14/26(0.54%)\n",
      "good matches:21/1000\n",
      "accuracy: 13/21(0.62%)\n",
      "good matches:23/1000\n",
      "accuracy: 12/23(0.52%)\n",
      "good matches:16/1000\n",
      "accuracy: 10/16(0.62%)\n",
      "good matches:24/1000\n",
      "accuracy: 9/24(0.38%)\n",
      "good matches:28/1000\n",
      "accuracy: 11/28(0.39%)\n",
      "good matches:29/1000\n",
      "accuracy: 12/29(0.41%)\n",
      "good matches:29/1000\n",
      "accuracy: 12/29(0.41%)\n",
      "good matches:16/1000\n",
      "accuracy: 7/16(0.44%)\n",
      "good matches:24/1000\n",
      "accuracy: 10/24(0.42%)\n",
      "good matches:23/1000\n",
      "accuracy: 14/23(0.61%)\n",
      "good matches:19/1000\n",
      "accuracy: 10/19(0.53%)\n",
      "good matches:21/1000\n",
      "accuracy: 12/21(0.57%)\n",
      "good matches:24/1000\n",
      "accuracy: 10/24(0.42%)\n",
      "good matches:17/1000\n",
      "accuracy: 11/17(0.65%)\n",
      "good matches:21/1000\n",
      "accuracy: 8/21(0.38%)\n",
      "good matches:20/1000\n",
      "accuracy: 12/20(0.60%)\n",
      "good matches:15/1000\n",
      "accuracy: 7/15(0.47%)\n",
      "good matches:17/1000\n",
      "accuracy: 10/17(0.59%)\n",
      "good matches:18/1000\n",
      "accuracy: 8/18(0.44%)\n",
      "good matches:23/1000\n",
      "accuracy: 10/23(0.43%)\n",
      "good matches:25/1000\n",
      "accuracy: 12/25(0.48%)\n",
      "good matches:21/1000\n",
      "accuracy: 11/21(0.52%)\n",
      "good matches:27/1000\n",
      "accuracy: 15/27(0.56%)\n",
      "good matches:18/1000\n",
      "accuracy: 11/18(0.61%)\n",
      "good matches:16/1000\n",
      "accuracy: 6/16(0.38%)\n",
      "good matches:18/1000\n",
      "accuracy: 11/18(0.61%)\n",
      "good matches:23/1000\n",
      "accuracy: 14/23(0.61%)\n",
      "good matches:17/1000\n",
      "accuracy: 14/17(0.82%)\n",
      "good matches:21/1000\n",
      "accuracy: 9/21(0.43%)\n",
      "good matches:16/1000\n",
      "accuracy: 11/16(0.69%)\n",
      "good matches:19/1000\n",
      "accuracy: 8/19(0.42%)\n",
      "good matches:29/1000\n",
      "accuracy: 10/29(0.34%)\n",
      "good matches:23/1000\n",
      "accuracy: 9/23(0.39%)\n",
      "good matches:24/1000\n",
      "accuracy: 12/24(0.50%)\n",
      "good matches:14/1000\n",
      "accuracy: 8/14(0.57%)\n",
      "good matches:17/1000\n",
      "accuracy: 7/17(0.41%)\n",
      "good matches:25/1000\n",
      "accuracy: 9/25(0.36%)\n",
      "good matches:19/1000\n",
      "accuracy: 9/19(0.47%)\n",
      "good matches:20/1000\n",
      "accuracy: 8/20(0.40%)\n",
      "good matches:11/1000\n",
      "accuracy: 5/11(0.45%)\n",
      "good matches:8/1000\n",
      "good matches:11/1000\n",
      "accuracy: 5/11(0.45%)\n",
      "good matches:11/1000\n",
      "accuracy: 6/11(0.55%)\n",
      "good matches:15/1000\n",
      "accuracy: 6/15(0.40%)\n",
      "good matches:22/1000\n",
      "accuracy: 8/22(0.36%)\n",
      "good matches:16/1000\n",
      "accuracy: 7/16(0.44%)\n",
      "good matches:18/1000\n",
      "accuracy: 7/18(0.39%)\n",
      "good matches:10/1000\n",
      "good matches:11/1000\n",
      "accuracy: 5/11(0.45%)\n",
      "good matches:22/1000\n",
      "accuracy: 9/22(0.41%)\n",
      "good matches:12/1000\n",
      "accuracy: 6/12(0.50%)\n",
      "good matches:18/1000\n",
      "accuracy: 5/18(0.28%)\n",
      "good matches:14/1000\n",
      "accuracy: 7/14(0.50%)\n",
      "good matches:22/1000\n",
      "accuracy: 8/22(0.36%)\n",
      "good matches:19/1000\n",
      "accuracy: 8/19(0.42%)\n",
      "good matches:14/1000\n",
      "accuracy: 5/14(0.36%)\n",
      "good matches:20/1000\n",
      "accuracy: 11/20(0.55%)\n",
      "good matches:23/1000\n",
      "accuracy: 9/23(0.39%)\n",
      "good matches:19/1000\n",
      "accuracy: 8/19(0.42%)\n",
      "good matches:13/1000\n",
      "accuracy: 8/13(0.62%)\n",
      "good matches:25/1000\n",
      "accuracy: 16/25(0.64%)\n",
      "good matches:27/1000\n",
      "accuracy: 16/27(0.59%)\n",
      "good matches:18/1000\n",
      "accuracy: 11/18(0.61%)\n",
      "good matches:22/1000\n",
      "accuracy: 13/22(0.59%)\n",
      "good matches:21/1000\n",
      "accuracy: 9/21(0.43%)\n",
      "good matches:19/1000\n",
      "accuracy: 8/19(0.42%)\n",
      "good matches:22/1000\n",
      "accuracy: 8/22(0.36%)\n",
      "good matches:25/1000\n",
      "accuracy: 10/25(0.40%)\n",
      "good matches:19/1000\n",
      "accuracy: 9/19(0.47%)\n",
      "good matches:22/1000\n",
      "accuracy: 10/22(0.45%)\n",
      "good matches:27/1000\n",
      "accuracy: 11/27(0.41%)\n",
      "good matches:17/1000\n",
      "accuracy: 9/17(0.53%)\n",
      "good matches:21/1000\n",
      "accuracy: 9/21(0.43%)\n",
      "good matches:22/1000\n",
      "accuracy: 11/22(0.50%)\n",
      "good matches:15/1000\n",
      "accuracy: 9/15(0.60%)\n",
      "good matches:24/1000\n",
      "accuracy: 15/24(0.62%)\n",
      "good matches:22/1000\n",
      "accuracy: 10/22(0.45%)\n",
      "good matches:15/1000\n",
      "accuracy: 6/15(0.40%)\n",
      "good matches:19/1000\n",
      "accuracy: 8/19(0.42%)\n",
      "good matches:18/1000\n",
      "accuracy: 9/18(0.50%)\n",
      "good matches:14/1000\n",
      "accuracy: 5/14(0.36%)\n",
      "good matches:28/1000\n",
      "accuracy: 8/28(0.29%)\n",
      "good matches:30/1000\n",
      "accuracy: 9/30(0.30%)\n",
      "good matches:22/1000\n",
      "accuracy: 6/22(0.27%)\n",
      "good matches:16/1000\n",
      "accuracy: 5/16(0.31%)\n",
      "good matches:16/1000\n",
      "accuracy: 7/16(0.44%)\n",
      "good matches:13/1000\n",
      "accuracy: 4/13(0.31%)\n",
      "good matches:20/1000\n",
      "accuracy: 6/20(0.30%)\n",
      "good matches:23/1000\n",
      "accuracy: 7/23(0.30%)\n",
      "good matches:15/1000\n",
      "accuracy: 4/15(0.27%)\n",
      "good matches:16/1000\n",
      "accuracy: 5/16(0.31%)\n",
      "good matches:13/1000\n",
      "accuracy: 5/13(0.38%)\n",
      "good matches:23/1000\n",
      "accuracy: 9/23(0.39%)\n",
      "good matches:21/1000\n",
      "accuracy: 6/21(0.29%)\n",
      "good matches:8/1000\n",
      "good matches:10/1000\n",
      "good matches:15/1000\n",
      "accuracy: 4/15(0.27%)\n",
      "good matches:16/1000\n",
      "accuracy: 5/16(0.31%)\n",
      "good matches:9/1000\n",
      "good matches:20/1000\n",
      "accuracy: 7/20(0.35%)\n",
      "good matches:13/1000\n",
      "accuracy: 5/13(0.38%)\n",
      "good matches:14/1000\n",
      "accuracy: 5/14(0.36%)\n",
      "good matches:25/1000\n",
      "accuracy: 7/25(0.28%)\n",
      "good matches:20/1000\n",
      "accuracy: 8/20(0.40%)\n",
      "good matches:21/1000\n",
      "accuracy: 7/21(0.33%)\n",
      "good matches:21/1000\n",
      "accuracy: 9/21(0.43%)\n",
      "good matches:14/1000\n",
      "accuracy: 5/14(0.36%)\n",
      "good matches:19/1000\n",
      "accuracy: 7/19(0.37%)\n",
      "good matches:17/1000\n",
      "accuracy: 7/17(0.41%)\n",
      "good matches:21/1000\n",
      "accuracy: 10/21(0.48%)\n",
      "good matches:5/1000\n",
      "good matches:16/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 7/16(0.44%)\n",
      "good matches:19/1000\n",
      "accuracy: 8/19(0.42%)\n",
      "good matches:17/1000\n",
      "accuracy: 6/17(0.35%)\n",
      "good matches:40/1000\n",
      "accuracy: 10/40(0.25%)\n",
      "good matches:47/1000\n",
      "accuracy: 19/47(0.40%)\n",
      "good matches:42/1000\n",
      "accuracy: 10/42(0.24%)\n",
      "good matches:43/1000\n",
      "accuracy: 15/43(0.35%)\n",
      "good matches:22/1000\n",
      "accuracy: 6/22(0.27%)\n",
      "good matches:25/1000\n",
      "accuracy: 15/25(0.60%)\n",
      "good matches:21/1000\n",
      "accuracy: 12/21(0.57%)\n",
      "good matches:30/1000\n",
      "accuracy: 15/30(0.50%)\n",
      "good matches:31/1000\n",
      "accuracy: 17/31(0.55%)\n",
      "good matches:31/1000\n",
      "accuracy: 14/31(0.45%)\n",
      "good matches:29/1000\n",
      "accuracy: 14/29(0.48%)\n",
      "good matches:31/1000\n",
      "accuracy: 18/31(0.58%)\n",
      "good matches:27/1000\n",
      "accuracy: 14/27(0.52%)\n",
      "good matches:27/1000\n",
      "accuracy: 15/27(0.56%)\n",
      "good matches:27/1000\n",
      "accuracy: 11/27(0.41%)\n",
      "good matches:30/1000\n",
      "accuracy: 10/30(0.33%)\n",
      "good matches:25/1000\n",
      "accuracy: 10/25(0.40%)\n",
      "good matches:39/1000\n",
      "accuracy: 21/39(0.54%)\n",
      "good matches:24/1000\n",
      "accuracy: 12/24(0.50%)\n",
      "good matches:24/1000\n",
      "accuracy: 11/24(0.46%)\n",
      "good matches:24/1000\n",
      "accuracy: 11/24(0.46%)\n",
      "good matches:27/1000\n",
      "accuracy: 10/27(0.37%)\n",
      "good matches:46/1000\n",
      "accuracy: 11/46(0.24%)\n",
      "good matches:24/1000\n",
      "accuracy: 8/24(0.33%)\n",
      "good matches:28/1000\n",
      "accuracy: 7/28(0.25%)\n",
      "good matches:16/1000\n",
      "accuracy: 5/16(0.31%)\n",
      "good matches:19/1000\n",
      "accuracy: 6/19(0.32%)\n",
      "good matches:29/1000\n",
      "accuracy: 13/29(0.45%)\n",
      "good matches:24/1000\n",
      "accuracy: 7/24(0.29%)\n",
      "good matches:15/1000\n",
      "accuracy: 7/15(0.47%)\n",
      "good matches:22/1000\n",
      "accuracy: 10/22(0.45%)\n",
      "good matches:23/1000\n",
      "accuracy: 7/23(0.30%)\n",
      "good matches:17/1000\n",
      "accuracy: 6/17(0.35%)\n",
      "good matches:19/1000\n",
      "accuracy: 7/19(0.37%)\n",
      "good matches:30/1000\n",
      "accuracy: 9/30(0.30%)\n",
      "good matches:28/1000\n",
      "accuracy: 7/28(0.25%)\n",
      "good matches:23/1000\n",
      "accuracy: 8/23(0.35%)\n",
      "good matches:25/1000\n",
      "accuracy: 7/25(0.28%)\n",
      "good matches:19/1000\n",
      "accuracy: 6/19(0.32%)\n",
      "good matches:17/1000\n",
      "accuracy: 5/17(0.29%)\n",
      "good matches:24/1000\n",
      "accuracy: 6/24(0.25%)\n",
      "good matches:13/1000\n",
      "accuracy: 4/13(0.31%)\n",
      "good matches:19/1000\n",
      "accuracy: 5/19(0.26%)\n",
      "good matches:18/1000\n",
      "accuracy: 7/18(0.39%)\n",
      "good matches:20/1000\n",
      "accuracy: 5/20(0.25%)\n",
      "good matches:19/1000\n",
      "accuracy: 6/19(0.32%)\n",
      "good matches:20/1000\n",
      "accuracy: 6/20(0.30%)\n",
      "good matches:13/1000\n",
      "accuracy: 4/13(0.31%)\n",
      "good matches:13/1000\n",
      "accuracy: 5/13(0.38%)\n",
      "good matches:18/1000\n",
      "accuracy: 5/18(0.28%)\n",
      "good matches:26/1000\n",
      "accuracy: 7/26(0.27%)\n",
      "good matches:21/1000\n",
      "accuracy: 5/21(0.24%)\n",
      "good matches:24/1000\n",
      "accuracy: 8/24(0.33%)\n",
      "good matches:23/1000\n",
      "accuracy: 6/23(0.26%)\n",
      "good matches:25/1000\n",
      "accuracy: 7/25(0.28%)\n"
     ]
    }
   ],
   "source": [
    "import cv2, numpy as np\n",
    "\n",
    "#img1 = None\n",
    "img1 = cv2.imread('./img/figures.jpg')\n",
    "win_name = 'Camera Matching'\n",
    "MIN_MATCH = 10\n",
    "# ORB 검출기 생성  ---①\n",
    "detector = cv2.ORB_create(1000)\n",
    "# Flann 추출기 생성 ---②\n",
    "FLANN_INDEX_LSH = 6\n",
    "index_params= dict(algorithm = FLANN_INDEX_LSH,\n",
    "                   table_number = 6,\n",
    "                   key_size = 12,\n",
    "                   multi_probe_level = 1)\n",
    "search_params=dict(checks=32)\n",
    "matcher = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "# 카메라 캡쳐 연결 및 프레임 크기 축소 ---③\n",
    "cap = cv2.VideoCapture(0)              \n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "while cap.isOpened():       \n",
    "    ret, frame = cap.read() \n",
    "    if img1 is None:  # 등록된 이미지 없음, 카메라 바이패스\n",
    "        res = frame\n",
    "    else:             # 등록된 이미지 있는 경우, 매칭 시작\n",
    "        img2 = frame\n",
    "        gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "        gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "        # 키포인트와 디스크립터 추출\n",
    "        kp1, desc1 = detector.detectAndCompute(gray1, None)\n",
    "        kp2, desc2 = detector.detectAndCompute(gray2, None)\n",
    "        # k=2로 knnMatch\n",
    "        matches = matcher.knnMatch(desc1, desc2, 2)\n",
    "        # 이웃 거리의 75%로 좋은 매칭점 추출---②\n",
    "        ratio = 0.75\n",
    "        good_matches = [m[0] for m in matches \\\n",
    "                if len(m) == 2 and m[0].distance < m[1].distance * ratio]\n",
    "        print('good matches:%d/%d' %(len(good_matches),len(matches)))\n",
    "        # 모든 매칭점 그리지 못하게 마스크를 0으로 채움\n",
    "        matchesMask = np.zeros(len(good_matches)).tolist()\n",
    "        # 좋은 매칭점 최소 갯수 이상 인 경우\n",
    "        if len(good_matches) > MIN_MATCH: \n",
    "            # 좋은 매칭점으로 원본과 대상 영상의 좌표 구하기 ---③\n",
    "            src_pts = np.float32([ kp1[m.queryIdx].pt for m in good_matches ])\n",
    "            dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good_matches ])\n",
    "            # 원근 변환 행렬 구하기 ---⑤\n",
    "            mtrx, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "            accuracy=float(mask.sum()) / mask.size\n",
    "            print(\"accuracy: %d/%d(%.2f%%)\"% (mask.sum(), mask.size, accuracy))\n",
    "            if mask.sum() > MIN_MATCH:  # 정상치 매칭점 최소 갯수 이상 인 경우\n",
    "                # 이상점 매칭점만 그리게 마스크 설정\n",
    "                matchesMask = mask.ravel().tolist()\n",
    "                # 원본 영상 좌표로 원근 변환 후 영역 표시  ---⑦\n",
    "                h,w, = img1.shape[:2]\n",
    "                pts = np.float32([ [[0,0]],[[0,h-1]],[[w-1,h-1]],[[w-1,0]] ])\n",
    "                dst = cv2.perspectiveTransform(pts,mtrx)\n",
    "                img2 = cv2.polylines(img2,[np.int32(dst)],True,255,3, cv2.LINE_AA)\n",
    "        # 마스크로 매칭점 그리기 ---⑨\n",
    "        res = cv2.drawMatches(img1, kp1, img2, kp2, good_matches, None, \\\n",
    "                            matchesMask=matchesMask,\n",
    "                            flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "    # 결과 출력\n",
    "    cv2.imshow(win_name, res)\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 27:    # Esc, 종료\n",
    "            break          \n",
    "    elif key == ord(' '): # 스페이스바를 누르면 ROI로 img1 설정\n",
    "        x,y,w,h = cv2.selectROI(win_name, frame, False)\n",
    "        if w and h:\n",
    "            img1 = frame[y:y+h, x:x+w]\n",
    "else:\n",
    "    print(\"can't open camera.\")\n",
    "cap.release()                          \n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e784b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#객체 추적\n",
    "#pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e2d89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "cv2.createBackgroundSubtractorMOG2([, history[, varthreshold[, detectShadows]]])\n",
    "  -history=500:히스토리 개수\n",
    "  -varThreshold=16:분산 임계 값\n",
    "  -detectShadows=True:그림자 표시\n",
    "\n",
    "지브고비치 알고리즘을 구현한 BackgroundSubtractorMOG2는 각 픽셀의 적절한 가우시안 \n",
    "분포값을 선택하므로 빛에 대한 변화가 많은 장면에 적합\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd1249ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BackgroundSubtractorMOG2 배경제거\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture('./img/walking.avi')\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)# 프레임 수 구하기\n",
    "delay = int(1000/fps)\n",
    "#배경제거 객체 생성\n",
    "fgbg = cv2.createBackgroundSubtractorMOG2()\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    #배경 제거 마스크 계산\n",
    "    fgmask = fgbg.apply(frame)\n",
    "    cv2.imshow('frame', frame)\n",
    "    cv2.imshow('bgsub', fgmask)\n",
    "    if cv2.waitKey(delay) & 0xff ==27:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2724cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#옵티컬 플로\n",
    "\"\"\"\n",
    "이전 장면과 다음 장면 사이의 픽셀이 이동한 방향과 거리에 대한 분포\n",
    "이를 통해 영상속 물체가 어느 방향으로 얼마만큼 움직였는지를 알수 있어\n",
    "움직임 자체에 대한 인식은 물론 여기에 추가적인 연산을 가하면 움직임을 예측할 수도 있음\n",
    "\n",
    "옵티컬 플로는 계산하는 방식에 따라 크게 두가지로 나뉨\n",
    "일부 픽셀만들 계산하는 희소 옵티컬 플로와 \n",
    "영상 전체 픽셀을 모두 계산하는 밀집 옵티컬 플로 (이건 잘 안씀)\n",
    "\n",
    "희소방식의 루카스 카나데 알고리즘을 구현한 cv2.calcOpticalFlowPyrLK 함수와 \n",
    "밀집 방식의 군나르 파너백 cv2.calcOpticalFlowFarneback함수\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f11326",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "cv2.calcOpticalFlowPyrLK함수는 픽셀 전체를 계산하지 않고 \n",
    "cv2.goodFeaturesToTrack함수로 얻은 코너 특징점만가지고 계싼.\n",
    "prevImg에 각각 이전 이후 프레임을 전달하고 prevPts에 이전 프레임에서 검출한 코너 \n",
    "특징점을 전달하면 그 코너점이 이후 프레임의 어디로 이동했는지 찾아서 nextPts로 반환\n",
    "이 함수는 작은 윈도를 사용하므로 큰 움직임을 계산하지 못하는 문제 존재.\n",
    "이를 보완하기 위해 이미지 피라미드 사용\n",
    "maxLevel에 0을 지정하면 이미지 피라미드를 사용하지 않음\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28b41633",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calcOpticalFlowPyrLK추적(희소방식)\n",
    "import numpy as np, cv2\n",
    "\n",
    "cap = cv2.VideoCapture('./img/walking.avi')\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)# 프레임 수 구하기\n",
    "delay = int(1000/fps)\n",
    "\n",
    "#추적 경로를 그리기 위한 랜덤 색상\n",
    "color = np.random.randint(0,255,(200,3))\n",
    "lines = None #추적 선을 그릴 이미지 저장 변수\n",
    "prevImg =None # 이전 프레임 저장 변수\n",
    "#calcOpticalFlowPyrLK 중지 요건 설정\n",
    "termcriteria = (cv2.TERM_CRITERIA_EPS| cv2.TERM_CRITERIA_COUNT, 10, 0.03)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    img_draw = frame.copy()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    #최초의 프레임의 경우\n",
    "    if prevImg is None:\n",
    "        prevImg = gray\n",
    "        #추적선을 그릴 이미지를 프레임 크기에 맞게 생성\n",
    "        lines = np.zeros_like(frame)\n",
    "        #추적 시작을 위한 코너 검출\n",
    "        prevPt = cv2.goodFeaturesToTrack(prevImg, 200, 0.01, 10)\n",
    "    else:\n",
    "        nextImg = gray\n",
    "        #옵티컬 플로로 다음 프레임의 코너점 찾기\n",
    "        nextPt, status, err = cv2.calcOpticalFlowPyrLK(prevImg, nextImg, prevPt, None, criteria =termcriteria)\n",
    "        #대응점이 있는 코너, 움직인 코너 선별\n",
    "        prevMv = prevPt[status==1]\n",
    "        nextMv = nextPt[status==1]\n",
    "        for i, (p,n) in enumerate(zip(prevMv, nextMv)):\n",
    "            px,py =p.ravel()\n",
    "            nx,ny =n.ravel()\n",
    "            #이전 코너와 새로운 코너에 선 그리기\n",
    "            #기존코드\n",
    "            #cv2.line(lines, (px, py), (nx,ny), color[i].tolist(), 2)\n",
    "            #이쪽 부분 int로 바꿔줘야 에러 안뜸\n",
    "            cv2.line(lines, (int(px), int(py)), (int(nx),int(ny)), color[i].tolist(), 2)\n",
    "            # 새로운 코너에 점 그리기\n",
    "            cv2.circle(img_draw, (int(nx),int(ny)), 2, color[i].tolist(), -1)\n",
    "\n",
    "        #누적된 추적 선을 출력이미지에 합성\n",
    "        img_draw = cv2.add(img_draw, lines)\n",
    "        #다음 프레임을 위한 프레임과 코너점 이월\n",
    "        prevImg = nextImg\n",
    "        prevPt = nextMv.reshape(-1,1,2)\n",
    "        \n",
    "    cv2.imshow('OpticalFlow-LK', img_draw)\n",
    "    key = cv2.waitKey(delay)\n",
    "    if key == 27: #esc종료\n",
    "        break\n",
    "    elif key == 8: #backspace:추적이력 지우기\n",
    "        prevImg==None\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9033d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CamShift추적\n",
    "\"\"\"\n",
    "MeanShift 추적의 문제점인 고정된 윈도 크기와 방향을 개선한 것으로 윈도 크기와 방향을 재설정\n",
    "추적을 위한 사전 작업과 절차는 MeanShift 추적과 동일하고 평균 중심점을 찾는 함수만 다름.\n",
    "\n",
    "retval, window = cv2.CamShift(probImage, window, criteria)\n",
    " -모든 인자와 반환 값은 MeanShift와 동일\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2db3342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n",
      "no cam\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2.cv2' has no attribute 'calHist'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28248/2876792768.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[1;31m#roi에 대한 히스토그램 계산\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mroi_hist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalHist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mroi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m180\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m180\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m             \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroi_hist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroi_hist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNORM_MINMAX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#roi설정 안됨\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cv2.cv2' has no attribute 'calHist'"
     ]
    }
   ],
   "source": [
    "#camShift 객체 추적\n",
    "import numpy as np, cv2\n",
    "\n",
    "roi_hist = None #추적 객체 히스토그램 저장 변수\n",
    "win_name = 'CamShift Tracking'\n",
    "termination = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    img_draw = frame.copy()\n",
    "    \n",
    "    if roi_hist is not None: #추적 대상 객체 히스토그램 등록됨\n",
    "        #전체 영상 hsv컬러로 변환\n",
    "        hsv=cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        #전체 영상 히스토그램과 roi 히스토그램 역투영\n",
    "        dst = cv2.calcBackProject([hsv], [0], roi_hist, [0,180], 1)\n",
    "        #역투영 결과와 초기 추적 위치로 평균 이동 추적\n",
    "        ret, (x,y,w,h) = cv2.CamShift(dst, (x,y,w,h), termination)\n",
    "        #새로운 위치에 사각형 표시\n",
    "        cv2.rectangle(img_draw, (x,y), (x+w, y+h), (0,255,0), 2)\n",
    "        #컬러 영상과 역투영 영상을 통합해서 출력\n",
    "        result = np.hstack((img_draw, cv2.cvtColor(dst, cv2.COLOR_GRAY2BGR)))\n",
    "    else: #추적 대상 객체 히스토그램 등록 안됨\n",
    "        cv2.putText(img_draw, 'Hit the Space to set target to track',\n",
    "                   (10,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 1, cv2.LINE_AA)\n",
    "        result=img_draw\n",
    "    \n",
    "    cv2.imshow(win_name, result)\n",
    "    key = cv2.waitKey(1) & 0xff\n",
    "    if key == 27: #esc\n",
    "        break\n",
    "    elif key == ord(' '): #스페이스바, ROI설정\n",
    "        x,y,w,h = cv2.selectROI(win_name, frame, False)\n",
    "        if w and h: #ROI가 제대로 설정됨\n",
    "            #초기 추적 대상 위치로 roi설정\n",
    "            roi = frame[y:y+h, x:x+w]\n",
    "            #roi를 HSV컬러로 변경\n",
    "            roi = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "            mask=None\n",
    "            #roi에 대한 히스토그램 계산\n",
    "            roi_hist = cv2.calHist([roi], [0], mask, [180], [0,180])\n",
    "            cv2.normalize(roi_hist, roi_hist, 0, 255, cv2.NORM_MINMAX)\n",
    "        else: #roi설정 안됨\n",
    "            roi_hist = None\n",
    "            \n",
    "    else:\n",
    "        print('no cam')\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "            \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eb31de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bb2478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd52797",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bee7cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524a6354",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e99d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83506ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a43838f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f330b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04098df4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de17e737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb2973e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cff9cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310cc7cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7908a3e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932192e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69aa5a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b60f24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaa78ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61757ac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e538eb93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061e2102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4525d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930819d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9bd81f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e81c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6590fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
